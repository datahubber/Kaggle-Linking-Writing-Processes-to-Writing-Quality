{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c5e2497-bbb8-4656-9f45-4feaa836121b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd#导入csv文件的库\n",
    "import numpy as np#进行矩阵运算的库\n",
    "import polars as pl#和pandas类似,但是处理大型数据集有更好的性能.\n",
    "#用于对一组元素计数,一个存在默认值的字典,访问不存在的值时抛出的是默认值\n",
    "from collections import Counter,defaultdict\n",
    "import re#用于正则表达式提取\n",
    "from scipy.stats import skew, kurtosis#统计分析和概率分布导入偏度和峰度\n",
    "import gc#垃圾回收模块\n",
    "\n",
    "#model\n",
    "from lightgbm import LGBMRegressor#导入lgbm回归器\n",
    "from catboost import CatBoostRegressor#catboost回归器\n",
    "\n",
    "#KFold是直接分成k折,StratifiedKFold还要考虑每种类别的占比\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler#用最大值和最小值进行归一化操作(x-min)/(max-min)\n",
    "\n",
    "#设置随机种子,保证模型可以复现\n",
    "import random\n",
    "seed=2023\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "import warnings#避免一些可以忽略的报错\n",
    "warnings.filterwarnings('ignore')#filterwarnings()方法是用于设置警告过滤器的方法，它可以控制警告信息的输出方式和级别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5bbb3b3-7fcc-4c44-9eec-6bde182fa6ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.11.5 (main, Sep 11 2023, 08:31:25) [Clang 14.0.6 ]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec43b3d7-a6e4-49c8-a018-01075caf9a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#交叉验证的折数\n",
    "num_folds = 5\n",
    "#融合模型的权重\n",
    "blending_weights = {\n",
    "    'lgbm': 0.3,\n",
    "    'catboost': 0.3,\n",
    "    'lightautoml': 0.3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a4c845f-ffa7-401d-b5cd-74a1fcd1357c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_logs):8405898\n",
      "len(test_logs):6\n"
     ]
    }
   ],
   "source": [
    "## Do it on train\n",
    "INPUT = \"/kaggle/input/linking-writing-processes-to-writing-quality\"\n",
    "train_logs=pd.read_csv(\"train_logs.csv\")\n",
    "print(f\"len(train_logs):{len(train_logs)}\")\n",
    "train_logs=train_logs.sort_values(by=['id', 'down_time'])\n",
    "# 重置索引\n",
    "train_logs = train_logs.reset_index(drop=True)\n",
    "# 根据'id'列进行分组，并为每个分组添加一个递增的序列\n",
    "train_logs['event_id'] = train_logs.groupby('id').cumcount() + 1\n",
    "\n",
    "train_scores=pd.read_csv(\"train_scores.csv\")\n",
    "\n",
    "# DO it on test\n",
    "test_logs=pd.read_csv(\"test_logs.csv\")\n",
    "print(f\"len(test_logs):{len(test_logs)}\")\n",
    "test_logs=test_logs.sort_values(by=['id', 'down_time'])\n",
    "# 重置索引\n",
    "test_logs = test_logs.reset_index(drop=True)\n",
    "# 根据'id'列进行分组，并为每个分组添加一个递增的序列\n",
    "test_logs['event_id'] = test_logs.groupby('id').cumcount() + 1\n",
    "test_logs.to_csv(\"test_logs.csv\",index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e038b87-0068-44c9-b34c-a5b45552aae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#统计‘q’ '.' ‘ ’的一个函数.\n",
    "def getEssays(df):\n",
    "    #获取传入的df的这几列.\n",
    "    textInputDf = df[['id', 'activity', 'cursor_position', 'text_change','down_event']]\n",
    "    #取出activity不等于'Nonproduction'的那些数据\n",
    "    textInputDf = textInputDf[textInputDf.activity != 'Nonproduction']\n",
    "    #统计每个id的出现次数,不排序\n",
    "    valCountsArr = textInputDf['id'].value_counts(sort=False).values\n",
    "    #最后的下标\n",
    "    lastIndex = 0\n",
    "    #创建一个新的序列对象.\n",
    "    essay_df = pd.DataFrame(columns=['essay'])\n",
    "    #index是第几个id,valCount是出现次数\n",
    "    for index, valCount in enumerate(valCountsArr):\n",
    "        #取出第i个id的['activity', 'cursor_position', 'text_change']\n",
    "        currTextInput = textInputDf[['activity', 'cursor_position', 'text_change','down_event']].iloc[lastIndex : lastIndex + valCount]\n",
    "        #跳到下一个id的index\n",
    "        lastIndex += valCount\n",
    "        essayText = \"\"\n",
    "        previousText = \"\"\n",
    "        for Input in currTextInput.values:\n",
    "            if Input[3] != 'z' or Input[0] != 'Remove/Cut':\n",
    "                previousText = essayText\n",
    "            #input[0]是这个id的activity\n",
    "            if Input[0] == 'Replace':\n",
    "                #text_change按照' => '分开 replaceTxt:[' qqq qqqqq ', ' ']\n",
    "                replaceTxt = Input[2].split(' => ')#应该是A=>B的操作\n",
    "                #input[1]是鼠标位置,是一个数字 鼠标位置-len()\n",
    "                #这是一个字符串的转换操作,由replaceTxt[0]转成replaceTxt[1] \n",
    "                essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] +essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n",
    "                continue\n",
    "            if Input[0] == 'Paste':#粘贴\n",
    "                #print(f\"input[2]:{Input[2]}\") #input[2]:qqqqqqqqqqq \n",
    "                essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n",
    "                continue\n",
    "            if Input[0] == 'Remove/Cut':#删除剪切 在Input[1]的位置删除Input[2]\n",
    "                if Input[3] == 'z':\n",
    "                    essayText = previousText\n",
    "                else:\n",
    "                    essayText = essayText[:Input[1]] + essayText[Input[1] + len(Input[2]):]\n",
    "                continue\n",
    "            #如果是Move from\n",
    "            if \"M\" in Input[0]:\n",
    "                #[284, 292] To [282, 290] 把[284, 292]这8行移动到[282,290]\n",
    "                croppedTxt = Input[0][10:]\n",
    "                #from和to的4个数字分开.\n",
    "                splitTxt = croppedTxt.split(' To ')\n",
    "                valueArr = [item.split(', ') for item in splitTxt]\n",
    "                moveData = (int(valueArr[0][0][1:]), \n",
    "                            int(valueArr[0][1][:-1]), \n",
    "                            int(valueArr[1][0][1:]), \n",
    "                            int(valueArr[1][1][:-1]))\n",
    "                #行号不相等,如果相等,等于什么都没有做\n",
    "                if moveData[0] != moveData[2]:\n",
    "                    #行号小于 \n",
    "                    if moveData[0] < moveData[2]:\n",
    "                        essayText = essayText[:moveData[0]] + essayText[moveData[1]:moveData[3]] +\\\n",
    "                        essayText[moveData[0]:moveData[1]] + essayText[moveData[3]:]\n",
    "                    #行号大于\n",
    "                    else:\n",
    "                        essayText = essayText[:moveData[2]] + essayText[moveData[0]:moveData[1]] +\\\n",
    "                        essayText[moveData[2]:moveData[0]] + essayText[moveData[1]:]\n",
    "                continue\n",
    "            #相当于是个check    \n",
    "            essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n",
    "        #对应id对应论文\n",
    "        essay_df = essay_df.append({'essay': essayText}, ignore_index=True)\n",
    "    #id\n",
    "    essay_df[\"id\"] =  textInputDf['id'].unique()\n",
    "    # return pd.DataFrame(essaySeries, columns=['essay']).reset_index().rename(columns={\"index\":'id'})\n",
    "    return essay_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e92508a0-4329-49e9-88e4-7462b8d02b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取数据中第25%的数值\n",
    "def q1(x):\n",
    "    return x.quantile(0.25)\n",
    "#获取数据中第75%的数值\n",
    "def q3(x):\n",
    "    return x.quantile(0.75)\n",
    "AGGREGATIONS = ['count', 'mean', 'std', 'min', 'max', 'first', 'last', 'sem', q1, 'median', q3, 'skew', kurtosis, 'sum']\n",
    "\n",
    "#将论文转成单词\n",
    "def split_essays_into_words(df):\n",
    "    essay_df = df\n",
    "    #对空格,\\n,句号问号感叹号进行匹配,得到一个拆分后的列表.\n",
    "    essay_df['word'] = essay_df['essay'].apply(lambda x: re.split(' |\\\\n|\\\\.|\\\\?|\\\\!',x))\n",
    "    # essay1 [1,2,3] essay2[4,5] ->5行 essay1 1  // essay1 2 // essay1 3 // essay2 1 // essay2 2\n",
    "    essay_df = essay_df.explode('word')\n",
    "    #求出每个单词的长度\n",
    "    essay_df['word_len'] = essay_df['word'].apply(lambda x: len(x))\n",
    "    #去掉单词长度为0的数据\n",
    "    essay_df = essay_df[essay_df['word_len'] != 0]\n",
    "    return essay_df \n",
    "\n",
    "#计算word_len的统计学变量,并计算>=word_len的词数\n",
    "def compute_word_aggregations(word_df):\n",
    "    #根据id计算单词长度的统计学变量\n",
    "    word_agg_df = word_df[['id','word_len']].groupby(['id']).agg(AGGREGATIONS)\n",
    "    #比如('mean','word_len')->'mean_word_len'\n",
    "    word_agg_df.columns = ['_'.join(x) for x in word_agg_df.columns]\n",
    "    word_agg_df['id'] = word_agg_df.index\n",
    "    for word_l in [5, 6, 7, 8, 9, 10, 11, 12]:\n",
    "        #ge 就是Latex里>=的符号,筛选出word_len>=word_l的行,根据id进行统计,提取每个计数的第0行\n",
    "        word_agg_df[f'word_len_ge_{word_l}_count'] = word_df[word_df['word_len'] >= word_l].groupby(['id']).count().iloc[:, 0]\n",
    "        #如果有缺失值就填充为0\n",
    "        word_agg_df[f'word_len_ge_{word_l}_count'] = word_agg_df[f'word_len_ge_{word_l}_count'].fillna(0)\n",
    "    #重置索引\n",
    "    word_agg_df = word_agg_df.reset_index(drop=True)\n",
    "    return word_agg_df\n",
    "\n",
    "#explore the punctuations\n",
    "def punctuations_agg(df):\n",
    "    essay_df = df.copy()\n",
    "    essay_df['between_punc'] = essay_df['essay'].apply(lambda x: re.split(r'[,\\n.?!]', x))\n",
    "    essay_df = essay_df.explode('between_punc')\n",
    "    essay_df['between_punc_len'] = essay_df['between_punc'].apply(lambda x: len(x))\n",
    "    punc_agg_df = essay_df[['id','between_punc_len']].groupby(['id']).agg(AGGREGATIONS)\n",
    "    punc_agg_df.columns = ['_'.join(x) for x in punc_agg_df.columns]\n",
    "    punc_agg_df['id'] = punc_agg_df.index\n",
    "    punc_agg_df.reset_index(drop=True, inplace=True)\n",
    "    return punc_agg_df\n",
    "\n",
    "def punctuations(df):\n",
    "    essay_df = df.copy()\n",
    "    punc_df = pd.DataFrame()\n",
    "    punc_df['exclamation_count'] = essay_df['essay'].apply(lambda x: x.count(\"!\"))\n",
    "    punc_df['period_count'] = essay_df['essay'].apply(lambda x: x.count(\".\"))\n",
    "    punc_df['question_count'] = essay_df['essay'].apply(lambda x: x.count(\"?\"))\n",
    "    punc_df['comma_per_sent'] = essay_df['essay'].apply(lambda x: x.count(\",\") / len(re.split(r'[\\n.?!]', x)))\n",
    "    return punc_df\n",
    "\n",
    "#将传入的论文df转成句子\n",
    "def split_essays_into_sentences(df):\n",
    "    essay_df = df#传入的df就是论文的df\n",
    "    #对句子按照. ? !进行拆分. 得到一个拆分后的列表.\n",
    "    essay_df['sent'] = essay_df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n",
    "    # essay1 [1,2,3] essay2[4,5] ->5行 essay1 1  // essay1 2 // essay1 3 // essay2 1 // essay2 2\n",
    "    essay_df = essay_df.explode('sent')\n",
    "    #将换行符'\\n'变成空白字符 strip 去除行头和行尾的空白字符.\n",
    "    essay_df['sent'] = essay_df['sent'].apply(lambda x: x.replace('\\n','').strip())\n",
    "    #统计一下每个句子的长度 \n",
    "    essay_df['sent_len'] = essay_df['sent'].apply(lambda x: len(x))\n",
    "    #求一下每个句子单词的个数.\n",
    "    essay_df['sent_word_count'] = essay_df['sent'].apply(lambda x: len(x.split(' ')))\n",
    "    #去掉那些句子长度为0的数据\n",
    "    essay_df = essay_df[essay_df.sent_len!=0].reset_index(drop=True)\n",
    "    return essay_df\n",
    "\n",
    "def compute_sentence_aggregations(df):\n",
    "    #统计句子长度的统计学变量和每个句子词数的统计学变量\n",
    "    sent_agg_df = pd.concat(\n",
    "        [df[['id','sent_len']].groupby(['id']).agg(AGGREGATIONS), df[['id','sent_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1\n",
    "    )\n",
    "    #比如('mean','sent_len')->'mean_sent_len'\n",
    "    sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n",
    "    sent_agg_df['id'] = sent_agg_df.index\n",
    "\n",
    "    # New features intoduced here: https://www.kaggle.com/code/mcpenguin/writing-processes-to-quality-baseline-v2\n",
    "    for sent_l in [50, 60, 75, 100]:\n",
    "        #ge 就是Latex里>=的符号,筛选出sent_len>=sent_l的行,根据id进行统计,提取每个计数的第0行\n",
    "        sent_agg_df[f'sent_len_ge_{sent_l}_count'] = df[df['sent_len'] >= sent_l].groupby(['id']).count().iloc[:, 0]\n",
    "        #如果有缺失值就填充为0\n",
    "        sent_agg_df[f'sent_len_ge_{sent_l}_count'] = sent_agg_df[f'sent_len_ge_{sent_l}_count'].fillna(0)\n",
    "    #重置索引\n",
    "    sent_agg_df = sent_agg_df.reset_index(drop=True)\n",
    "    #一句话里词的个数的count,其实就是有多少句话,也就是sent_len的count.重复了,故去掉.\n",
    "    sent_agg_df.drop(columns=[\"sent_word_count_count\"], inplace=True)\n",
    "    #sent_len_count其实就是有多少句话,故rename.\n",
    "    sent_agg_df = sent_agg_df.rename(columns={\"sent_len_count\":\"sent_count\"})\n",
    "    return sent_agg_df\n",
    "\n",
    "#将论文根据换行符划分为段落.(每段有多少句话为什么没有统计?)\n",
    "def split_essays_into_paragraphs(df):\n",
    "    essay_df = df\n",
    "    #按照'\\n'划分成段落 [1,2,3]\n",
    "    essay_df['paragraph'] = essay_df['essay'].apply(lambda x: x.split('\\n'))\n",
    "    #[论文1 [段落1 段落2,……]->[论文1 段落1 // 论文1 段落2]\n",
    "    essay_df = essay_df.explode('paragraph')\n",
    "    #统计段落的长度\n",
    "    essay_df['paragraph_len'] = essay_df['paragraph'].apply(lambda x: len(x)) \n",
    "    #统计每个段落的词数\n",
    "    essay_df['paragraph_word_count'] = essay_df['paragraph'].apply(lambda x: len(x.split(' ')))\n",
    "    #将段落长度为0的数据去掉.\n",
    "    essay_df = essay_df[essay_df.paragraph_len!=0].reset_index(drop=True)\n",
    "    return essay_df\n",
    "\n",
    "#对段落的长度和词数用统计学变量,和上面句子的代码一致.\n",
    "def compute_paragraph_aggregations(df):\n",
    "    paragraph_agg_df = pd.concat(\n",
    "        [df[['id','paragraph_len']].groupby(['id']).agg(AGGREGATIONS), df[['id','paragraph_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1\n",
    "    ) \n",
    "    paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n",
    "    paragraph_agg_df['id'] = paragraph_agg_df.index\n",
    "    paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n",
    "    paragraph_agg_df.drop(columns=[\"paragraph_word_count_count\"], inplace=True)\n",
    "    paragraph_agg_df = paragraph_agg_df.rename(columns={\"paragraph_len_count\":\"paragraph_count\"})\n",
    "    return paragraph_agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33f0e12f-b36e-4b81-89b5-33a0371bf14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_essays\n",
      "train_punc_agg_df\n",
      "train_word_agg_df\n",
      "train_sent_agg_df\n",
      "train_paragraph_agg_df\n",
      "test_essays\n",
      "train_punc_agg_df\n",
      "test_word_agg_df\n",
      "test_sent_agg_df\n",
      "test_paragraph_agg_df\n"
     ]
    }
   ],
   "source": [
    "print(\"train_essays\")\n",
    "# train_essays = pd.read_csv('yunsu_essay.csv')\n",
    "train_essays = getEssays(train_logs)\n",
    "# train_essays.to_csv(\"yunsu_essay.csv\",index=False)\n",
    "print(\"train_punc_agg_df\")\n",
    "train_punc_agg_df = punctuations_agg(train_essays)\n",
    "train_punc_agg_df = pd.concat([train_punc_agg_df,punctuations(train_essays)],axis=1)\n",
    "print(\"train_word_agg_df\")\n",
    "train_word_agg_df = compute_word_aggregations(split_essays_into_words(train_essays))\n",
    "print(\"train_sent_agg_df\")\n",
    "train_sent_agg_df = compute_sentence_aggregations(split_essays_into_sentences(train_essays))\n",
    "print(\"train_paragraph_agg_df\")\n",
    "train_paragraph_agg_df = compute_paragraph_aggregations(split_essays_into_paragraphs(train_essays))\n",
    "\n",
    "print(\"test_essays\")\n",
    "test_essays = getEssays(test_logs)\n",
    "test_essays_copy=test_essays.copy()\n",
    "print(\"train_punc_agg_df\")\n",
    "test_punc_agg_df = punctuations_agg(test_essays)\n",
    "test_punc_agg_df = pd.concat([test_punc_agg_df,punctuations(test_essays)],axis=1)\n",
    "print(\"test_word_agg_df\")\n",
    "test_word_agg_df = compute_word_aggregations(split_essays_into_words(test_essays))\n",
    "print(\"test_sent_agg_df\")\n",
    "test_sent_agg_df = compute_sentence_aggregations(split_essays_into_sentences(test_essays))\n",
    "print(\"test_paragraph_agg_df\")\n",
    "test_paragraph_agg_df = compute_paragraph_aggregations(split_essays_into_paragraphs(test_essays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d9b92a1-6c3d-4875-bc1b-890a03849f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering features for training data\n",
      "Starting to engineer features\n",
      "Engineering time data\n",
      "-> for gap 1\n",
      "-> for gap 2\n",
      "-> for gap 3\n",
      "-> for gap 5\n",
      "-> for gap 10\n",
      "-> for gap 20\n",
      "-> for gap 50\n",
      "-> for gap 100\n",
      "Engineering cursor position data\n",
      "-> for gap 1\n",
      "-> for gap 2\n",
      "-> for gap 3\n",
      "-> for gap 5\n",
      "-> for gap 10\n",
      "-> for gap 20\n",
      "-> for gap 50\n",
      "-> for gap 100\n",
      "Engineering word count data\n",
      "-> for gap 1\n",
      "-> for gap 2\n",
      "-> for gap 3\n",
      "-> for gap 5\n",
      "-> for gap 10\n",
      "-> for gap 20\n",
      "-> for gap 50\n",
      "-> for gap 100\n",
      "Engineering statistical summaries for features\n",
      "Engineering activity counts data\n",
      "Engineering event counts data\n",
      "Engineering text change counts data\n",
      "Engineering punctuation counts data\n",
      "Engineering input words data\n",
      "Engineering ratios data\n",
      "Engineer R/P burst (new)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>event_id</th>\n",
       "      <th>down_time</th>\n",
       "      <th>up_time</th>\n",
       "      <th>action_time</th>\n",
       "      <th>activity</th>\n",
       "      <th>down_event</th>\n",
       "      <th>up_event</th>\n",
       "      <th>text_change</th>\n",
       "      <th>cursor_position</th>\n",
       "      <th>...</th>\n",
       "      <th>word_count_change5</th>\n",
       "      <th>word_count_abs_change5</th>\n",
       "      <th>word_count_change10</th>\n",
       "      <th>word_count_abs_change10</th>\n",
       "      <th>word_count_change20</th>\n",
       "      <th>word_count_abs_change20</th>\n",
       "      <th>word_count_change50</th>\n",
       "      <th>word_count_abs_change50</th>\n",
       "      <th>word_count_change100</th>\n",
       "      <th>word_count_abs_change100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001519c8</td>\n",
       "      <td>1</td>\n",
       "      <td>4526</td>\n",
       "      <td>4557</td>\n",
       "      <td>31</td>\n",
       "      <td>Nonproduction</td>\n",
       "      <td>Leftclick</td>\n",
       "      <td>Leftclick</td>\n",
       "      <td>NoChange</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001519c8</td>\n",
       "      <td>2</td>\n",
       "      <td>4558</td>\n",
       "      <td>4962</td>\n",
       "      <td>404</td>\n",
       "      <td>Nonproduction</td>\n",
       "      <td>Leftclick</td>\n",
       "      <td>Leftclick</td>\n",
       "      <td>NoChange</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001519c8</td>\n",
       "      <td>3</td>\n",
       "      <td>106571</td>\n",
       "      <td>106571</td>\n",
       "      <td>0</td>\n",
       "      <td>Nonproduction</td>\n",
       "      <td>Shift</td>\n",
       "      <td>Shift</td>\n",
       "      <td>NoChange</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001519c8</td>\n",
       "      <td>4</td>\n",
       "      <td>106686</td>\n",
       "      <td>106777</td>\n",
       "      <td>91</td>\n",
       "      <td>Input</td>\n",
       "      <td>q</td>\n",
       "      <td>q</td>\n",
       "      <td>q</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001519c8</td>\n",
       "      <td>5</td>\n",
       "      <td>107196</td>\n",
       "      <td>107323</td>\n",
       "      <td>127</td>\n",
       "      <td>Input</td>\n",
       "      <td>q</td>\n",
       "      <td>q</td>\n",
       "      <td>q</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8405893</th>\n",
       "      <td>fff05981</td>\n",
       "      <td>3615</td>\n",
       "      <td>2063944</td>\n",
       "      <td>2064440</td>\n",
       "      <td>496</td>\n",
       "      <td>Nonproduction</td>\n",
       "      <td>Leftclick</td>\n",
       "      <td>Leftclick</td>\n",
       "      <td>NoChange</td>\n",
       "      <td>1031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8405894</th>\n",
       "      <td>fff05981</td>\n",
       "      <td>3616</td>\n",
       "      <td>2064497</td>\n",
       "      <td>2064497</td>\n",
       "      <td>0</td>\n",
       "      <td>Nonproduction</td>\n",
       "      <td>Shift</td>\n",
       "      <td>Shift</td>\n",
       "      <td>NoChange</td>\n",
       "      <td>1031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8405895</th>\n",
       "      <td>fff05981</td>\n",
       "      <td>3617</td>\n",
       "      <td>2064657</td>\n",
       "      <td>2064765</td>\n",
       "      <td>108</td>\n",
       "      <td>Replace</td>\n",
       "      <td>q</td>\n",
       "      <td>q</td>\n",
       "      <td>q =&gt; q</td>\n",
       "      <td>1031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8405896</th>\n",
       "      <td>fff05981</td>\n",
       "      <td>3618</td>\n",
       "      <td>2069186</td>\n",
       "      <td>2069259</td>\n",
       "      <td>73</td>\n",
       "      <td>Nonproduction</td>\n",
       "      <td>Leftclick</td>\n",
       "      <td>Leftclick</td>\n",
       "      <td>NoChange</td>\n",
       "      <td>1028</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8405897</th>\n",
       "      <td>fff05981</td>\n",
       "      <td>3619</td>\n",
       "      <td>2070065</td>\n",
       "      <td>2070133</td>\n",
       "      <td>68</td>\n",
       "      <td>Input</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>1029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8405898 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id  event_id  down_time  up_time  action_time       activity  \\\n",
       "0        001519c8         1       4526     4557           31  Nonproduction   \n",
       "1        001519c8         2       4558     4962          404  Nonproduction   \n",
       "2        001519c8         3     106571   106571            0  Nonproduction   \n",
       "3        001519c8         4     106686   106777           91          Input   \n",
       "4        001519c8         5     107196   107323          127          Input   \n",
       "...           ...       ...        ...      ...          ...            ...   \n",
       "8405893  fff05981      3615    2063944  2064440          496  Nonproduction   \n",
       "8405894  fff05981      3616    2064497  2064497            0  Nonproduction   \n",
       "8405895  fff05981      3617    2064657  2064765          108        Replace   \n",
       "8405896  fff05981      3618    2069186  2069259           73  Nonproduction   \n",
       "8405897  fff05981      3619    2070065  2070133           68          Input   \n",
       "\n",
       "        down_event   up_event text_change  cursor_position  ...  \\\n",
       "0        Leftclick  Leftclick    NoChange                0  ...   \n",
       "1        Leftclick  Leftclick    NoChange                0  ...   \n",
       "2            Shift      Shift    NoChange                0  ...   \n",
       "3                q          q           q                1  ...   \n",
       "4                q          q           q                2  ...   \n",
       "...            ...        ...         ...              ...  ...   \n",
       "8405893  Leftclick  Leftclick    NoChange             1031  ...   \n",
       "8405894      Shift      Shift    NoChange             1031  ...   \n",
       "8405895          q          q      q => q             1031  ...   \n",
       "8405896  Leftclick  Leftclick    NoChange             1028  ...   \n",
       "8405897          .          .           .             1029  ...   \n",
       "\n",
       "         word_count_change5  word_count_abs_change5  word_count_change10  \\\n",
       "0                       NaN                     NaN                  NaN   \n",
       "1                       NaN                     NaN                  NaN   \n",
       "2                       NaN                     NaN                  NaN   \n",
       "3                       NaN                     NaN                  NaN   \n",
       "4                       NaN                     NaN                  NaN   \n",
       "...                     ...                     ...                  ...   \n",
       "8405893                 0.0                     0.0                 -1.0   \n",
       "8405894                 0.0                     0.0                  0.0   \n",
       "8405895                 0.0                     0.0                  0.0   \n",
       "8405896                 0.0                     0.0                  0.0   \n",
       "8405897                 0.0                     0.0                  0.0   \n",
       "\n",
       "         word_count_abs_change10  word_count_change20  \\\n",
       "0                            NaN                  NaN   \n",
       "1                            NaN                  NaN   \n",
       "2                            NaN                  NaN   \n",
       "3                            NaN                  NaN   \n",
       "4                            NaN                  NaN   \n",
       "...                          ...                  ...   \n",
       "8405893                      1.0                 -1.0   \n",
       "8405894                      0.0                  0.0   \n",
       "8405895                      0.0                  0.0   \n",
       "8405896                      0.0                  0.0   \n",
       "8405897                      0.0                 -1.0   \n",
       "\n",
       "         word_count_abs_change20  word_count_change50  \\\n",
       "0                            NaN                  NaN   \n",
       "1                            NaN                  NaN   \n",
       "2                            NaN                  NaN   \n",
       "3                            NaN                  NaN   \n",
       "4                            NaN                  NaN   \n",
       "...                          ...                  ...   \n",
       "8405893                      1.0                  2.0   \n",
       "8405894                      0.0                  2.0   \n",
       "8405895                      0.0                  2.0   \n",
       "8405896                      0.0                  1.0   \n",
       "8405897                      1.0                  1.0   \n",
       "\n",
       "         word_count_abs_change50  word_count_change100  \\\n",
       "0                            NaN                   NaN   \n",
       "1                            NaN                   NaN   \n",
       "2                            NaN                   NaN   \n",
       "3                            NaN                   NaN   \n",
       "4                            NaN                   NaN   \n",
       "...                          ...                   ...   \n",
       "8405893                      2.0                  11.0   \n",
       "8405894                      2.0                  10.0   \n",
       "8405895                      2.0                  10.0   \n",
       "8405896                      1.0                  10.0   \n",
       "8405897                      1.0                  10.0   \n",
       "\n",
       "         word_count_abs_change100  \n",
       "0                             NaN  \n",
       "1                             NaN  \n",
       "2                             NaN  \n",
       "3                             NaN  \n",
       "4                             NaN  \n",
       "...                           ...  \n",
       "8405893                      11.0  \n",
       "8405894                      10.0  \n",
       "8405895                      10.0  \n",
       "8405896                      10.0  \n",
       "8405897                      10.0  \n",
       "\n",
       "[8405898 rows x 51 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thinking Time (new)\n",
      "wpm (new)\n",
      "Done!\n",
      "-------------------------\n",
      "Engineering features for test data\n",
      "Starting to engineer features\n",
      "Engineering time data\n",
      "-> for gap 1\n",
      "-> for gap 2\n",
      "-> for gap 3\n",
      "-> for gap 5\n",
      "-> for gap 10\n",
      "-> for gap 20\n",
      "-> for gap 50\n",
      "-> for gap 100\n",
      "Engineering cursor position data\n",
      "-> for gap 1\n",
      "-> for gap 2\n",
      "-> for gap 3\n",
      "-> for gap 5\n",
      "-> for gap 10\n",
      "-> for gap 20\n",
      "-> for gap 50\n",
      "-> for gap 100\n",
      "Engineering word count data\n",
      "-> for gap 1\n",
      "-> for gap 2\n",
      "-> for gap 3\n",
      "-> for gap 5\n",
      "-> for gap 10\n",
      "-> for gap 20\n",
      "-> for gap 50\n",
      "-> for gap 100\n",
      "Engineering statistical summaries for features\n",
      "Engineering activity counts data\n",
      "Engineering event counts data\n",
      "Engineering text change counts data\n",
      "Engineering punctuation counts data\n",
      "Engineering input words data\n",
      "Engineering ratios data\n",
      "Engineer R/P burst (new)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>event_id</th>\n",
       "      <th>down_time</th>\n",
       "      <th>up_time</th>\n",
       "      <th>action_time</th>\n",
       "      <th>activity</th>\n",
       "      <th>down_event</th>\n",
       "      <th>up_event</th>\n",
       "      <th>text_change</th>\n",
       "      <th>cursor_position</th>\n",
       "      <th>...</th>\n",
       "      <th>word_count_change5</th>\n",
       "      <th>word_count_abs_change5</th>\n",
       "      <th>word_count_change10</th>\n",
       "      <th>word_count_abs_change10</th>\n",
       "      <th>word_count_change20</th>\n",
       "      <th>word_count_abs_change20</th>\n",
       "      <th>word_count_change50</th>\n",
       "      <th>word_count_abs_change50</th>\n",
       "      <th>word_count_change100</th>\n",
       "      <th>word_count_abs_change100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000aaaa</td>\n",
       "      <td>1</td>\n",
       "      <td>338433</td>\n",
       "      <td>338518</td>\n",
       "      <td>85</td>\n",
       "      <td>Input</td>\n",
       "      <td>Space</td>\n",
       "      <td>Space</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000aaaa</td>\n",
       "      <td>2</td>\n",
       "      <td>760073</td>\n",
       "      <td>760160</td>\n",
       "      <td>87</td>\n",
       "      <td>Input</td>\n",
       "      <td>Space</td>\n",
       "      <td>Space</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2222bbbb</td>\n",
       "      <td>1</td>\n",
       "      <td>290502</td>\n",
       "      <td>290548</td>\n",
       "      <td>46</td>\n",
       "      <td>Input</td>\n",
       "      <td>q</td>\n",
       "      <td>q</td>\n",
       "      <td>q</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2222bbbb</td>\n",
       "      <td>2</td>\n",
       "      <td>711956</td>\n",
       "      <td>712023</td>\n",
       "      <td>67</td>\n",
       "      <td>Input</td>\n",
       "      <td>q</td>\n",
       "      <td>q</td>\n",
       "      <td>q</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4444cccc</td>\n",
       "      <td>1</td>\n",
       "      <td>184996</td>\n",
       "      <td>185052</td>\n",
       "      <td>56</td>\n",
       "      <td>Input</td>\n",
       "      <td>q</td>\n",
       "      <td>q</td>\n",
       "      <td>q</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4444cccc</td>\n",
       "      <td>2</td>\n",
       "      <td>635547</td>\n",
       "      <td>635641</td>\n",
       "      <td>94</td>\n",
       "      <td>Input</td>\n",
       "      <td>Space</td>\n",
       "      <td>Space</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  event_id  down_time  up_time  action_time activity down_event  \\\n",
       "0  0000aaaa         1     338433   338518           85    Input      Space   \n",
       "1  0000aaaa         2     760073   760160           87    Input      Space   \n",
       "2  2222bbbb         1     290502   290548           46    Input          q   \n",
       "3  2222bbbb         2     711956   712023           67    Input          q   \n",
       "4  4444cccc         1     184996   185052           56    Input          q   \n",
       "5  4444cccc         2     635547   635641           94    Input      Space   \n",
       "\n",
       "  up_event text_change  cursor_position  ...  word_count_change5  \\\n",
       "0    Space                            0  ...                 NaN   \n",
       "1    Space                            1  ...                 NaN   \n",
       "2        q           q                1  ...                 NaN   \n",
       "3        q           q                0  ...                 NaN   \n",
       "4        q           q                1  ...                 NaN   \n",
       "5    Space                            0  ...                 NaN   \n",
       "\n",
       "   word_count_abs_change5  word_count_change10  word_count_abs_change10  \\\n",
       "0                     NaN                  NaN                      NaN   \n",
       "1                     NaN                  NaN                      NaN   \n",
       "2                     NaN                  NaN                      NaN   \n",
       "3                     NaN                  NaN                      NaN   \n",
       "4                     NaN                  NaN                      NaN   \n",
       "5                     NaN                  NaN                      NaN   \n",
       "\n",
       "   word_count_change20  word_count_abs_change20  word_count_change50  \\\n",
       "0                  NaN                      NaN                  NaN   \n",
       "1                  NaN                      NaN                  NaN   \n",
       "2                  NaN                      NaN                  NaN   \n",
       "3                  NaN                      NaN                  NaN   \n",
       "4                  NaN                      NaN                  NaN   \n",
       "5                  NaN                      NaN                  NaN   \n",
       "\n",
       "   word_count_abs_change50  word_count_change100  word_count_abs_change100  \n",
       "0                      NaN                   NaN                       NaN  \n",
       "1                      NaN                   NaN                       NaN  \n",
       "2                      NaN                   NaN                       NaN  \n",
       "3                      NaN                   NaN                       NaN  \n",
       "4                      NaN                   NaN                       NaN  \n",
       "5                      NaN                   NaN                       NaN  \n",
       "\n",
       "[6 rows x 51 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thinking Time (new)\n",
      "wpm (new)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "class Preprocessor:#数据预处理的一个类\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste','Move From']#这是activity的一列\n",
    "        self.events = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', \n",
    "              'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']#down_event中选出一些重要的\n",
    "        self.text_changes = ['q', ' ', 'NoChange', '.', ',', '\\n', \"'\", '\"', '-', '?', ';', '=', '/', '\\\\', ':']#text_change中选出一些重要的\n",
    "        self.punctuations = ['\"', '.', ',', \"'\", '-', ';', ':', '?', '!', '<', '>', '/',\n",
    "                        '@', '#', '$', '%', '^', '&', '*', '(', ')', '_', '+']#down_event中的一些标点符号\n",
    "        self.gaps = [1, 2, 3, 5, 10, 20, 50, 100]#滞后项\n",
    "        \n",
    "        #这里是用于存储每个activity的idf值\n",
    "        self.idf = defaultdict(float)#创建了一个float类型的字典,如果访问不存在,默认值为0.0\n",
    "    \n",
    "    #统计df对象中activity的count\n",
    "    def activity_counts(self, df):\n",
    "        #对每个id的所有activity组合成一个列表\n",
    "        tmp_df = df.groupby('id').agg({'activity': list}).reset_index()\n",
    "        #创建一个空列表\n",
    "        ret = list()\n",
    "        for li in tmp_df['activity'].values:#取出一个人的activity列表\n",
    "            items = list(Counter(li).items())#转成[(activity1:count1),(activity2:count2),……]\n",
    "            di = dict()#一个空字典\n",
    "            #每个activity初始化为0\n",
    "            for k in self.activities:\n",
    "                di[k] = 0\n",
    "            #统计每个activity的count\n",
    "            for item in items:\n",
    "                k, v = item[0], item[1]#k:activity v:count\n",
    "                if k in di:\n",
    "                    di[k] = v\n",
    "            #加上这个人的每个activity的count\n",
    "            ret.append(di)\n",
    "        #转成pandas类型\n",
    "        ret = pd.DataFrame(ret)\n",
    "        #给表格的每列换个名字\n",
    "        cols = [f'activity_{i}_count' for i in range(len(ret.columns))]\n",
    "        ret.columns = cols\n",
    "\n",
    "        #每列元素求和,文章中出现的总次数\n",
    "        #add up the sum in each row\n",
    "        cnts = ret.sum(1)\n",
    "\n",
    "        #前面是词袋模型,这里转成tf-idf模型\n",
    "        for col in cols:#activity_i_count\n",
    "            if col in self.idf.keys():#字典里如果已经有这个key了\n",
    "                idf = self.idf[col]\n",
    "            else:#不在这个字典里\n",
    "                #计算idf=log(数据量/(某列和+1))\n",
    "                idf = np.log(df.shape[0] / (ret[col].sum() + 1))\n",
    "                self.idf[col] = idf#将col的idf加入字典\n",
    "            #ret[col] / cnts :一个entry的某种类型的总输入次数 / 一个entry该特征大类中所有类型,为什么取log再加1不知道\n",
    "            ret[col] = 1 + np.log(ret[col] / cnts)\n",
    "            ret[col] *= idf\n",
    "\n",
    "        return ret#tf-idf\n",
    "\n",
    "    #这个是event的tf-idf模型,这里可能有down_event和up_event,故colname单独设置\n",
    "    def event_counts(self, df, colname):\n",
    "        tmp_df = df.groupby('id').agg({colname: list}).reset_index()\n",
    "        ret = list()\n",
    "        for li in tmp_df[colname].values:\n",
    "            items = list(Counter(li).items())\n",
    "            di = dict()\n",
    "            for k in self.events:\n",
    "                di[k] = 0\n",
    "            for item in items:\n",
    "                k, v = item[0], item[1]\n",
    "                if k in di:\n",
    "                    di[k] = v\n",
    "            ret.append(di)\n",
    "        # a list of dictionary => df\n",
    "        # columns = possible values for event column\n",
    "        ret = pd.DataFrame(ret)\n",
    "        cols = [f'{colname}_{i}_count' for i in range(len(ret.columns))]\n",
    "        ret.columns = cols\n",
    "\n",
    "        cnts = ret.sum(1)\n",
    "\n",
    "        for col in cols:\n",
    "            if col in self.idf.keys():\n",
    "                idf = self.idf[col]\n",
    "            else:\n",
    "                idf = df.shape[0] / (ret[col].sum() + 1)\n",
    "                idf = np.log(idf)\n",
    "                self.idf[col] = idf\n",
    "            \n",
    "            ret[col] = 1 + np.log(ret[col] / cnts)\n",
    "            ret[col] *= idf\n",
    "\n",
    "        return ret\n",
    "\n",
    "    #text_change的tf-idf模型\n",
    "    def text_change_counts(self, df):\n",
    "        tmp_df = df.groupby('id').agg({'text_change': list}).reset_index()\n",
    "        ret = list()\n",
    "        for li in tmp_df['text_change'].values:\n",
    "            items = list(Counter(li).items())\n",
    "            di = dict()\n",
    "            for k in self.text_changes:\n",
    "                di[k] = 0\n",
    "            for item in items:\n",
    "                k, v = item[0], item[1]\n",
    "                if k in di:\n",
    "                    di[k] = v\n",
    "            ret.append(di)\n",
    "        ret = pd.DataFrame(ret)\n",
    "        cols = [f'text_change_{i}_count' for i in range(len(ret.columns))]\n",
    "        ret.columns = cols\n",
    "\n",
    "        cnts = ret.sum(1)\n",
    "\n",
    "        for col in cols:\n",
    "            if col in self.idf.keys():\n",
    "                idf = self.idf[col]\n",
    "            else:\n",
    "                idf = df.shape[0] / (ret[col].sum() + 1)\n",
    "                idf = np.log(idf)\n",
    "                self.idf[col] = idf\n",
    "            \n",
    "            ret[col] = 1 + np.log(ret[col] / cnts)\n",
    "            ret[col] *= idf\n",
    "            \n",
    "        return ret\n",
    "    #统计标点之类的出现的次数,不过这次是直接将它们相加做统计的.(可能这样比tf-idf好?)\n",
    "    def match_punctuations(self, df):\n",
    "        tmp_df = df.groupby('id').agg({'down_event': list}).reset_index()\n",
    "        ret = list()\n",
    "        for li in tmp_df['down_event'].values:\n",
    "            cnt = 0\n",
    "            items = list(Counter(li).items())\n",
    "            for item in items:\n",
    "                k, v = item[0], item[1]\n",
    "                if k in self.punctuations:#只要在这张表里,就相加\n",
    "                    cnt += v\n",
    "            ret.append(cnt)\n",
    "        ret = pd.DataFrame({'punct_cnt': ret})\n",
    "        return ret\n",
    "\n",
    "\n",
    "    def get_input_words(self, df):\n",
    "        #~是取反的布尔值 取出text_change 中不包含 => 且不是Nochange的\n",
    "        tmp_df = df[(~df['text_change'].str.contains('=>'))&(df['text_change'] != 'NoChange')].reset_index(drop=True)\n",
    "        #在drop掉包含 => 和Nochange之后 按id打包成列表\n",
    "        tmp_df = tmp_df.groupby('id').agg({'text_change': list}).reset_index()\n",
    "        #将列表连接成一个整体\n",
    "        tmp_df['text_change'] = tmp_df['text_change'].apply(lambda x: ''.join(x))\n",
    "        #用正则表达式子匹配一个或者多个'q'字符\n",
    "        tmp_df['text_change'] = tmp_df['text_change'].apply(lambda x: re.findall(r'q+', x))\n",
    "        #统计len,也就是统计text_change中有多少个有q的字符\n",
    "        tmp_df['input_word_count'] = tmp_df['text_change'].apply(len)\n",
    "        #求均值,方差,最大值,取到np.nan就设置为0\n",
    "        tmp_df['input_word_length_mean'] = tmp_df['text_change'].apply(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0))\n",
    "        tmp_df['input_word_length_max'] = tmp_df['text_change'].apply(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0))\n",
    "        tmp_df['input_word_length_std'] = tmp_df['text_change'].apply(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0))\n",
    "        tmp_df.drop(['text_change'], axis=1, inplace=True)\n",
    "        return tmp_df\n",
    "    \n",
    "    #对df做特征工程!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    def make_feats(self, df):\n",
    "        print(\"Starting to engineer features\")\n",
    "        #创建一个只有id一列的表格\n",
    "        feats = pd.DataFrame({'id': df['id'].unique().tolist()})\n",
    "        #做时序上的特征工程\n",
    "        print(\"Engineering time data\")\n",
    "        for gap in self.gaps:\n",
    "            print(f\"-> for gap {gap}\")\n",
    "            #利用up_time的shift创造action_time_gap\n",
    "            df[f'up_time_shift{gap}'] = df.groupby('id')['up_time'].shift(gap)\n",
    "            df[f'action_time_gap{gap}'] = df['down_time'] - df[f'up_time_shift{gap}']\n",
    "        df.drop(columns=[f'up_time_shift{gap}' for gap in self.gaps], inplace=True)\n",
    "\n",
    "        #对cursor_position做特征工程,这个就是自己-自己\n",
    "        print(\"Engineering cursor position data\")\n",
    "        for gap in self.gaps:\n",
    "            print(f\"-> for gap {gap}\")\n",
    "            df[f'cursor_position_shift{gap}'] = df.groupby('id')['cursor_position'].shift(gap)\n",
    "            df[f'cursor_position_change{gap}'] = df['cursor_position'] - df[f'cursor_position_shift{gap}']\n",
    "            #取了绝对值,鼠标向前移动也是移动了.\n",
    "            df[f'cursor_position_abs_change{gap}'] = np.abs(df[f'cursor_position_change{gap}'])\n",
    "        df.drop(columns=[f'cursor_position_shift{gap}' for gap in self.gaps], inplace=True)\n",
    "\n",
    "        #对word_count做类似的特征工程,词数减少也是移动了.\n",
    "        print(\"Engineering word count data\")\n",
    "        for gap in self.gaps:\n",
    "            print(f\"-> for gap {gap}\")\n",
    "            df[f'word_count_shift{gap}'] = df.groupby('id')['word_count'].shift(gap)\n",
    "            df[f'word_count_change{gap}'] = df['word_count'] - df[f'word_count_shift{gap}']\n",
    "            df[f'word_count_abs_change{gap}'] = np.abs(df[f'word_count_change{gap}'])\n",
    "        df.drop(columns=[f'word_count_shift{gap}' for gap in self.gaps], inplace=True)\n",
    "        \n",
    "        print(\"Engineering statistical summaries for features\")\n",
    "        #需要对哪些特征做哪些统计变量,这些都是大佬统计好的,就不做修改了.\n",
    "        feats_stat = [\n",
    "            ('event_id', ['max']),\n",
    "            ('down_time',['mean', 'std', 'min', 'max', 'last', 'first', 'sem', 'median', 'sum']),\n",
    "            ('up_time',['mean', 'std', 'min', 'max', 'last', 'first', 'sem', 'median', 'sum']),\n",
    "            ('action_time', ['max', 'min', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt,'last', 'first','median']),\n",
    "            ('activity', ['nunique']),\n",
    "            ('down_event', ['nunique']),\n",
    "            ('up_event', ['nunique']),\n",
    "            ('text_change', ['nunique']),\n",
    "            ('cursor_position', ['nunique', 'max', 'quantile', 'sem', 'mean', 'std', 'min','last', 'first',  'median', 'sum']),\n",
    "            ('word_count', ['nunique', 'max', 'quantile', 'sem', 'mean', 'std', 'min', 'last', 'first','median', 'sum'])]\n",
    "        #滞后特征的统计变量用for循环进行添加\n",
    "        for gap in self.gaps:\n",
    "            feats_stat.extend([\n",
    "                (f'action_time_gap{gap}', ['max', 'min', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt]),\n",
    "                (f'cursor_position_change{gap}', ['max', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt]),\n",
    "                (f'word_count_change{gap}', ['max', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt])\n",
    "            ])\n",
    "        \n",
    "        pbar = feats_stat\n",
    "        for item in pbar:\n",
    "            colname, methods = item[0], item[1]#取出某列特征和需要进行的统计学的量'max'\n",
    "            for method in methods:\n",
    "                #转成能放入agg的方法\n",
    "                if isinstance(method, str):\n",
    "                    method_name = method\n",
    "                else:\n",
    "                    method_name = method.__name__\n",
    "                #添加到feats里.\n",
    "                tmp_df = df.groupby(['id']).agg({colname: method}).reset_index().rename(columns={colname: f'{colname}_{method_name}'})\n",
    "                feats = feats.merge(tmp_df, on='id', how='left')\n",
    "\n",
    "        #调用方法求activity的tf-idf\n",
    "        print(\"Engineering activity counts data\")\n",
    "        tmp_df = self.activity_counts(df)\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "        #调用方法求down_event和up_event的tf-idf\n",
    "        print(\"Engineering event counts data\")\n",
    "        tmp_df = self.event_counts(df, 'down_event')\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "        tmp_df = self.event_counts(df, 'up_event')\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "        \n",
    "        print(\"Engineering text change counts data\")\n",
    "        tmp_df = self.text_change_counts(df)\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "        \n",
    "        print(\"Engineering punctuation counts data\")\n",
    "        tmp_df = self.match_punctuations(df)\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "\n",
    "        # input words\n",
    "        print(\"Engineering input words data\")\n",
    "        tmp_df = self.get_input_words(df)\n",
    "        feats = pd.merge(feats, tmp_df, on='id', how='left')\n",
    "\n",
    "        # compare feats\n",
    "        print(\"Engineering ratios data\")\n",
    "        feats['word_time_ratio'] = feats['word_count_max'] / feats['up_time_max']\n",
    "        feats['word_event_ratio'] = feats['word_count_max'] / feats['event_id_max']\n",
    "        feats['event_time_ratio'] = feats['event_id_max']  / feats['up_time_max']\n",
    "        #休息时间的占比\n",
    "        feats['idle_time_ratio'] = feats['action_time_gap1_sum'] / feats['up_time_max']\n",
    "\n",
    "        \n",
    "        print(\"Engineer R/P burst (new)\")\n",
    "        def helper(selected_burst): \n",
    "            selected_burst = selected_burst.reset_index(drop=True)\n",
    "            selected_burst[0] = False\n",
    "            end_indices = []\n",
    "            start_indices = []\n",
    "            for i in range(1,len(selected_burst)): \n",
    "                if selected_burst[i] == False and selected_burst[i-1] == True:\n",
    "                    end_indices.append(i)\n",
    "                elif selected_burst[i] == True and selected_burst[i-1] == False:\n",
    "                    start_indices.append(i)\n",
    "            return [end_indices[i] - start_indices[i] for i in range(len(end_indices))]\n",
    "           \n",
    "        #df is edited in the process, not drop anything orignal\n",
    "        tmp_df = df.copy()\n",
    "        display(tmp_df)\n",
    "        tmp_df = tmp_df[tmp_df['activity'].isin(['Remove/Cut','Input'])].reset_index(drop=True)\n",
    "        R_burst = tmp_df['activity'] == 'Input' \n",
    "        P_burst = tmp_df['action_time_gap1'] < 2000\n",
    "        valCountsArr = tmp_df['id'].value_counts(sort=False).items()\n",
    "        last_index = 0\n",
    "        burst_df = pd.DataFrame(columns=['id', 'P_burst', 'R_burst', 'PR_burst'])\n",
    "        for id, count in valCountsArr:\n",
    "            selected_P = P_burst.iloc[last_index:last_index + count]\n",
    "            selected_R = R_burst.iloc[last_index:last_index + count]\n",
    "            selected_P_len = helper(selected_P)\n",
    "            selected_R_len = helper(selected_R)\n",
    "            selected_PR_len = helper(selected_P & selected_R)\n",
    "            burst_df = burst_df.append({'id': id, \n",
    "                                        'P_burst': selected_P_len, \n",
    "                                        'R_burst': selected_R_len, \n",
    "                                        'PR_burst': selected_PR_len}, \n",
    "                                    ignore_index=True)\n",
    "            last_index += count\n",
    "            \n",
    "            \n",
    "        def help_agg_burst(burst_df, colname):\n",
    "            tmp_df = burst_df[['id',colname]].explode(colname)\n",
    "            tmp_df[colname] = pd.to_numeric(tmp_df[colname], errors='coerce')\n",
    "            tmp_df_agg = tmp_df.groupby(['id']).agg(AGGREGATIONS)\n",
    "            tmp_df_agg.columns = ['_'.join(x) for x in tmp_df_agg.columns]\n",
    "            return tmp_df_agg\n",
    "\n",
    "        burst_df[\"P_burst\"] = burst_df[\"P_burst\"].apply(lambda x : [num for num in x if num >= 5])\n",
    "        burst_df[\"R_burst\"] = burst_df[\"R_burst\"].apply(lambda x : [num for num in x if num >= 5])\n",
    "        burst_df[\"PR_burst\"] = burst_df[\"PR_burst\"].apply(lambda x : [num for num in x if num >= 5])\n",
    "        burst_df = pd.concat([help_agg_burst(burst_df, \"P_burst\"),help_agg_burst(burst_df, \"R_burst\"),help_agg_burst(burst_df, \"PR_burst\")],axis=1).reset_index()\n",
    "        feats = pd.merge(feats, burst_df, on='id', how='left')\n",
    "        \n",
    "        print(\"Thinking Time (new)\")\n",
    "        thinking_time = df[df.activity == 'Input'].groupby(['id']).first().reset_index()\n",
    "        feats = pd.concat([feats,thinking_time['down_time']],axis=1)\n",
    "        feats.rename(columns={\"down_time\":\"thinking_time\"},inplace=True)\n",
    "        \n",
    "        \n",
    "        print(\"wpm (new)\")\n",
    "        wpm_tmp_df = df.copy()\n",
    "        def helper_wpm(group):\n",
    "            return group['word_count'].iloc[-1] / group['up_time'].iloc[-1] * 60000\n",
    "        wpm_df = wpm_tmp_df[['word_count','up_time','id']].groupby(['id']).apply(helper_wpm).reset_index()\n",
    "        wpm_df.rename(columns = {0:\"wpm\"},inplace=True)\n",
    "        feats = pd.merge(feats, wpm_df, on='id', how='left')\n",
    "        \n",
    "        \n",
    "        print(\"Done!\")\n",
    "        return feats\n",
    "\n",
    "preprocessor = Preprocessor()\n",
    "print(\"Engineering features for training data\")\n",
    "\n",
    "train_logs_copy = train_logs.copy()\n",
    "test_logs_copy = test_logs.copy()\n",
    "train_feats = preprocessor.make_feats(train_logs_copy)\n",
    "print(\"-\"*25)\n",
    "print(\"Engineering features for test data\")\n",
    "test_feats = preprocessor.make_feats(test_logs_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f73b0289-7f5f-4229-8d20-410dc36ca85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for logs in [train_logs, test_logs]:\n",
    "    #up_time向后移动并且用down_time填充缺失的位置\n",
    "    logs['up_time_lagged'] = logs.groupby('id')['up_time'].shift(1).fillna(logs['down_time'])\n",
    "    #(down_time减上一个时刻的up_time) /1000是单位转换\n",
    "    logs['time_diff'] = abs(logs['down_time'] - logs['up_time_lagged']) / 1000\n",
    "\n",
    "    #按照id打包time_diff\n",
    "    group = logs.groupby('id')['time_diff']\n",
    "    #延迟时间的max,min,median\n",
    "    largest_lantency = group.max()\n",
    "    smallest_lantency = group.min()\n",
    "    median_lantency = group.median()\n",
    "    #down_time的first /1000是做单位转换吧\n",
    "    initial_pause = logs.groupby('id')['down_time'].first() / 1000\n",
    "    #分层次求和\n",
    "    pauses_half_sec = group.apply(lambda x: ((x > 0.5) & (x <= 1)).sum())\n",
    "    pauses_1_sec = group.apply(lambda x: ((x > 1) & (x <= 1.5)).sum())\n",
    "    pauses_1_half_sec = group.apply(lambda x: ((x > 1.5) & (x <= 2)).sum())\n",
    "    pauses_2_sec = group.apply(lambda x: ((x > 2) & (x <= 3)).sum())\n",
    "    pauses_3_sec = group.apply(lambda x: (x > 3).sum())\n",
    "\n",
    "    data.append(pd.DataFrame({\n",
    "        'id': logs['id'].unique(),\n",
    "         #延迟\n",
    "        'largest_lantency': largest_lantency,\n",
    "        'smallest_lantency': smallest_lantency,\n",
    "        'median_lantency': median_lantency,\n",
    "        'initial_pause': initial_pause,\n",
    "        'pauses_half_sec': pauses_half_sec,\n",
    "        'pauses_1_sec': pauses_1_sec,\n",
    "        'pauses_1_half_sec': pauses_1_half_sec,\n",
    "        'pauses_2_sec': pauses_2_sec,\n",
    "        'pauses_3_sec': pauses_3_sec,\n",
    "    }).reset_index(drop=True))\n",
    "\n",
    "train_eD592674, test_eD592674 = data\n",
    "\n",
    "gc.collect()#手动触发垃圾回收,强制回收由垃圾回收器标记为未使用的内存\n",
    "\n",
    "train_feats = train_feats.merge(train_eD592674, on='id', how='left')\n",
    "test_feats = test_feats.merge(test_eD592674, on='id', how='left')\n",
    "train_feats = train_feats.merge(train_scores, on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5949dae3-9742-4fcb-bf80-a4ab20fea1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#将论文的特征加上\n",
    "train_feats=train_feats.merge(train_word_agg_df,on='id', how='left')\n",
    "train_feats=train_feats.merge(train_sent_agg_df,on='id', how='left')\n",
    "train_feats=train_feats.merge(train_paragraph_agg_df,on='id', how='left')\n",
    "train_feats=train_feats.merge(train_punc_agg_df,on='id', how='left')\n",
    "## replace with actual data (new)\n",
    "train_feats['word_len_count'] = train_logs.groupby('id')['word_count'].last().reset_index(drop=True)\n",
    "#将论文的特征加上\n",
    "test_feats=test_feats.merge(test_word_agg_df,on='id', how='left')\n",
    "test_feats=test_feats.merge(test_sent_agg_df,on='id', how='left')\n",
    "test_feats=test_feats.merge(test_paragraph_agg_df,on='id', how='left')\n",
    "test_feats=test_feats.merge(test_punc_agg_df,on='id', how='left')\n",
    "test_feats['word_len_count'] = test_logs.groupby('id')['word_count'].last().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "051b1bd0-0361-49a8-8b47-5f6478e83689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_cols:['cursor_position_min', 'word_count_change1_quantile', 'word_count_change2_quantile', 'activity_5_count', 'smallest_lantency']\n"
     ]
    }
   ],
   "source": [
    "#找到只有唯一值的列,删掉\n",
    "keys=train_feats.keys().values\n",
    "unique_cols=[key for key in keys if train_feats[key].nunique()<2]\n",
    "print(f\"unique_cols:{unique_cols}\")\n",
    "train_feats = train_feats.drop(columns=unique_cols)\n",
    "test_feats = test_feats.drop(columns=unique_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d745c001-1cb5-44a6-b91c-28e138ca81b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features = train_feats.drop(['score'],axis=1).keys().values \n",
    "train_scores = train_feats[['id','score']]\n",
    "X_y = pd.merge(train_feats[best_features], train_scores, on='id', how='left')\n",
    "X_y.replace([np.inf, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a59d6122-3ab2-493b-843f-428a4a96a11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#这里创建了lgbm,cat,SVR模型\n",
    "def make_model():\n",
    "    \n",
    "    #大佬找好的参数,这里不做改动\n",
    "    params = {'reg_alpha': 0.007678095440286993, \n",
    "               'reg_lambda': 0.34230534302168353, \n",
    "               'colsample_bytree': 0.627061253588415, \n",
    "               'subsample': 0.854942238828458, \n",
    "               'learning_rate': 0.038697981947473245, \n",
    "               'num_leaves': 22, \n",
    "               'max_depth': 37, \n",
    "               'min_child_samples': 18,\n",
    "               'random_state': seed,\n",
    "               'n_estimators': 150,\n",
    "               \"objective\": \"regression\",\n",
    "               \"metric\": \"rmse\",\n",
    "               'force_col_wise': True,\n",
    "               \"verbosity\": 0,\n",
    "              }\n",
    "    \n",
    "    model1 = LGBMRegressor(**params)\n",
    "    \n",
    "    model2 = CatBoostRegressor(iterations=1000,\n",
    "                                 learning_rate=0.1,\n",
    "                                 depth=6,\n",
    "                                 eval_metric='RMSE',\n",
    "                                 random_seed = seed,\n",
    "                                 bagging_temperature = 0.2,\n",
    "                                 od_type='Iter',\n",
    "                                 metric_period = 50,\n",
    "                                 od_wait=20,\n",
    "                                 verbose=False)\n",
    "    \n",
    "    \n",
    "    models = []\n",
    "    models.append((model1, 'lgbm'))\n",
    "    models.append((model2, 'catboost'))\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3819c1-9034-4d4a-b962-1d03a024af98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib \n",
    "# solution 1 with gan\n",
    "# models_and_errors_dict = joblib.load(\"/kaggle/input/lgbm-cb-gan-pkl/lgbm_and_cb_model.pkl\")\n",
    "\n",
    "# solution 2 without gan\n",
    "models_and_errors_dict = joblib.load(\"/kaggle/input/lgbm-and-cb-pkl/lgbm_and_cb_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aeff213-f9a8-4305-a39d-0ee775fba377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.5 (main, Sep 11 2023, 08:31:25) [Clang 14.0.6 ]\n"
     ]
    }
   ],
   "source": [
    "conda "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72d784f5-f50e-4779-94db-009bb19b2c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Ignored the following versions that require a different python version: 0.3.0 Requires-Python >=3.6.1,<3.10; 0.3.1 Requires-Python >=3.6.1,<3.10; 0.3.2 Requires-Python >=3.6.1,<3.10; 0.3.3 Requires-Python >=3.6.1,<3.10; 0.3.4 Requires-Python >=3.6.1,<3.10; 0.3.5 Requires-Python >=3.6.1,<3.10; 0.3.6 Requires-Python >=3.6.1,<3.10; 0.3.7 Requires-Python >=3.6.1,<3.10; 0.3.7.1 Requires-Python >=3.6.1,<3.10; 0.3.7.2 Requires-Python >=3.6.1,<3.10; 0.3.7.3 Requires-Python >=3.6.1,<3.10; 0.3.8b1 Requires-Python >=3.6.1,<3.11\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement lightautoml==0.3.8b1 (from versions: 0.2.1, 0.2.2, 0.2.3, 0.2.4, 0.2.5, 0.2.6, 0.2.7, 0.2.8, 0.2.10, 0.2.11, 0.2.12, 0.2.13, 0.2.14, 0.2.15, 0.2.16)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for lightautoml==0.3.8b1\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install lightautoml==0.3.8b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1ad1966-b31a-4840-b978-5ebc2d8dce31",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2649740877.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    brew update\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "brew update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d5d1c03-da97-4917-a010-1517d6f72666",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightautoml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# LightAutoML presets, task and report generation\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightautoml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautoml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpresets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtabular_presets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TabularAutoML, TabularUtilizedAutoML\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightautoml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Task\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightautoml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreport\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreport_deco\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReportDeco\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lightautoml'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import pandas as pd\n",
    "# LightAutoML presets, task and report generation\n",
    "from lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML\n",
    "from lightautoml.tasks import Task\n",
    "from lightautoml.report.report_deco import ReportDeco\n",
    "\n",
    "N_THREADS = 4\n",
    "num_folds = 5\n",
    "# N_FOLDS = 5 redundant\n",
    "RANDOM_STATE = 2033 \n",
    "TEST_SIZE = 0.2 \n",
    "TIMEOUT = 3600\n",
    "TARGET_NAME = 'score'\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# task = Task('reg')\n",
    "\n",
    "# train_data = pd.read_csv(\"yunsu_train_feats.csv\")\n",
    "\n",
    "# roles = {\n",
    "# 'target': TARGET_NAME, 'drop': ['id']\n",
    "# }\n",
    "\n",
    "\n",
    "# automl = TabularAutoML(\n",
    "#     task = task,\n",
    "#     cpu_limit = N_THREADS,\n",
    "#     reader_params = {'n_jobs': N_THREADS, 'cv': num_folds, 'random_state': RANDOM_STATE, 'device': 'gpu'},\n",
    "#     timeout = TIMEOUT\n",
    "# )\n",
    "\n",
    "# out_of_fold_predictions = automl.fit_predict(train_data, roles = roles, verbose = 1)\n",
    "# test_predictions = automl.predict(test_feats)\n",
    "# print(f'Prediction for test_data:\\n{test_predictions}\\nShape = {test_predictions.shape}')\n",
    "# joblib.dump((automl, out_of_fold_predictions.data[:,0]),\"automl_model_and_predictions_yunsu.pkl\")\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "automl, oof_prediction = joblib.load(\"/kaggle/input/lightauoml-roy-pkl/Roy.pkl\")\n",
    "lightautoml_rmse = mean_squared_error(X_y['score'].values, oof_prediction,squared=False)\n",
    "print(f'OOF score (RMSE): {lightautoml_rmse}')\n",
    "models_and_errors_dict['lightautoml'] = [(automl, lightautoml_rmse, None, None,oof_prediction)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14af3caf-2282-4f95-a072-5cc399b22766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(y_true,y_pred):\n",
    "    return np.sqrt(np.mean((y_true-y_pred)**2))\n",
    "target = pd.DataFrame(X_y['score'],columns=['score'])\n",
    "\n",
    "lgb_oof_pred=models_and_errors_dict['lgbm'][4][4]\n",
    "cat_oof_pred=models_and_errors_dict['catboost'][4][4]\n",
    "lightautoml_oof_pred=models_and_errors_dict['lightautoml'][0][4]\n",
    "margin=100\n",
    "# 重运行就崩了，呜呜\n",
    "# target=target.values\n",
    "lgbm_RMSE = RMSE(target['score'],lgb_oof_pred)\n",
    "cat_RMSE = RMSE(target['score'],cat_oof_pred)\n",
    "lightautoml_RMSE = RMSE(target['score'],lightautoml_oof_pred)\n",
    "print(\n",
    "    f\"\"\"\n",
    "    The CV RMSE of lgbm: {lgbm_RMSE}\n",
    "    The CV RMSE of cb: {cat_RMSE}\n",
    "    The CV RMSE of lightautoml:{lightautoml_RMSE}\n",
    "    \"\"\"\n",
    ")\n",
    "current_RMSE=RMSE(target['score'],(lgb_oof_pred+cat_oof_pred)/2)\n",
    "best_i=0\n",
    "best_j=0\n",
    "for i in range(0,margin):\n",
    "    for j in range(0,margin-i):\n",
    "        #o=1000-i-j\n",
    "            blend_oof_pred=(i*lgb_oof_pred+j*cat_oof_pred+(margin-i-j)*lightautoml_oof_pred)/margin\n",
    "            if RMSE(target['score'],blend_oof_pred)<current_RMSE:\n",
    "                print(f\"current_RMSE:{current_RMSE}\")\n",
    "                current_RMSE=RMSE(target['score'],blend_oof_pred)\n",
    "                best_i=i\n",
    "                best_j=j\n",
    "\n",
    "#找到最好的参数之后\n",
    "blending_weights['lgbm']=best_i/margin\n",
    "blending_weights['catboost']=best_j/margin\n",
    "blending_weights['lightautoml']=(margin-best_i-best_j)/margin\n",
    "print(f\"blending_weights:{blending_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b084f6c-020e-4b8c-94d1-f7e612144f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "y_hats = dict()\n",
    "\n",
    "#设置submission_df,id和score\n",
    "submission_df = pd.DataFrame(test_feats['id'])\n",
    "submission_df['score'] = 3.5#如果报错,将预测结果设置为3.5\n",
    "\n",
    "#取出test_feats中所有列\n",
    "X_unseen = test_feats.copy()[best_features]\n",
    "X_unseen.drop(columns=['id'], inplace=True)\n",
    "X_unseen.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X_unseen.loc[:, X_unseen.isna().all()] = 0\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "X_unseen_imputed = imputer.fit_transform(X_unseen)\n",
    "X_unseen_imputed = pd.DataFrame(X_unseen_imputed, columns=X_unseen.columns)\n",
    "X_unseen = X_unseen_imputed\n",
    "\n",
    "\n",
    "for model_name, model_info in models_and_errors_dict.items():\n",
    "    print(f'\\n--- {model_name} ---\\n')\n",
    "    \n",
    "    #复制是因为有的要归一化\n",
    "    X_unseen_copy = X_unseen.copy()\n",
    "    y_hats[model_name] = []#某个model的预测结果\n",
    "\n",
    "    for ix, (trained_model, error, imputer, uhhh ,oof_pred) in enumerate(model_info, start=1):\n",
    "        print(f\"Using model {ix} with error {error}\")\n",
    "\n",
    "        if model_name == \"lightautoml\":\n",
    "            y_hats[model_name].append(trained_model.predict(X_unseen_copy).data[:,0])\n",
    "        else: y_hats[model_name].append(trained_model.predict(X_unseen_copy))\n",
    "        \n",
    "    #如果有值的话,求平均,赋值给submission_df\n",
    "    if y_hats[model_name]:\n",
    "        y_hat_avg = np.mean(y_hats[model_name], axis=0)\n",
    "        submission_df['score_' + model_name] = y_hat_avg\n",
    "    print(\"Done.\")\n",
    "    \n",
    "print(\"blending\")\n",
    "blended_score=np.zeros((len(test_essays_copy)))\n",
    "for k, v in blending_weights.items():\n",
    "    blended_score += submission_df['score_' + k] * v\n",
    "print(f\"blended_score:{blended_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a686cf-b4ca-4b2a-8619-c1c032107199",
   "metadata": {},
   "outputs": [],
   "source": [
    "#数值型变量的几列\n",
    "num_cols = ['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']\n",
    "activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n",
    "events = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', 'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']\n",
    "text_changes = ['q', ' ', '.', ',', '\\n', \"'\", '\"', '-', '?', ';', '=', '/', '\\\\', ':']\n",
    "\n",
    "#df表格的colname列统计values的count.\n",
    "def count_by_values(df, colname, values):\n",
    "    #maintain_order=True保持原有顺序\n",
    "    fts = df.select(pl.col('id').unique(maintain_order=True))\n",
    "    for i, value in enumerate(values):\n",
    "        #根据每个id判断colname是不是value并统计个数,rename成colname_i_cnt\n",
    "        tmp_df = df.group_by('id').agg(pl.col(colname).is_in([value]).sum().alias(f'{colname}_{i}_cnt'))\n",
    "        #加上这个特征\n",
    "        fts  = fts.join(tmp_df, on='id', how='left') \n",
    "    return fts\n",
    "\n",
    "def dev_feats(df):\n",
    "    \n",
    "    print(\"< Count by values features >\")\n",
    "    \n",
    "    #统计activity,text_change,down_event,up_event这几个类别型变量的count\n",
    "    feats = count_by_values(df, 'activity', activities)\n",
    "    feats = feats.join(count_by_values(df, 'text_change', text_changes), on='id', how='left') \n",
    "    feats = feats.join(count_by_values(df, 'down_event', events), on='id', how='left') \n",
    "    feats = feats.join(count_by_values(df, 'up_event', events), on='id', how='left') \n",
    "\n",
    "    print(\"< Input words stats features >\")\n",
    "    #不含有'=>'且有变化的行\n",
    "    temp = df.filter((~pl.col('text_change').str.contains('=>')) & (pl.col('text_change') != 'NoChange'))\n",
    "    #按照id将text_change连接成一个长字符串,然后匹配'q+'的字符串\n",
    "    temp = temp.group_by('id').agg(pl.col('text_change').str.concat('').str.extract_all(r'q+'))\n",
    "    #统计输入的词数,词长度的均值,最大值,方差,中位数,偏斜度.\n",
    "    temp = temp.with_columns(input_word_count = pl.col('text_change').list.lengths(),\n",
    "                             input_word_length_mean = pl.col('text_change').apply(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "                             input_word_length_max = pl.col('text_change').apply(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "                             input_word_length_std = pl.col('text_change').apply(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "                             input_word_length_median = pl.col('text_change').apply(lambda x: np.median([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "                             input_word_length_skew = pl.col('text_change').apply(lambda x: skew([len(i) for i in x] if len(x) > 0 else 0)))\n",
    "    #将text_change这列去掉,加入特征.\n",
    "    temp = temp.drop('text_change')\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "    print(\"< Numerical columns features >\")\n",
    "\n",
    "    #对action_time求和,对数值型变量求均值,方差,中位数,最小值,最大值,50%的数字\n",
    "    temp = df.group_by(\"id\").agg(pl.sum('action_time').suffix('_sum'), pl.mean(num_cols).suffix('_mean'), pl.std(num_cols).suffix('_std'),\n",
    "                                 pl.median(num_cols).suffix('_median'), pl.min(num_cols).suffix('_min'), pl.max(num_cols).suffix('_max'),\n",
    "                                 pl.quantile(num_cols, 0.5).suffix('_quantile'))\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "\n",
    "    print(\"< Categorical columns features >\")\n",
    "    #类别型变量求了n_unique,加入特征.\n",
    "    temp  = df.group_by(\"id\").agg(pl.n_unique(['activity', 'down_event', 'up_event', 'text_change']))\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "    print(\"< Idle time features >\")\n",
    "    #这里就是论文中的特征.(https://files.eric.ed.gov/fulltext/ED592674.pdf)\n",
    "    temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n",
    "    temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n",
    "    temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "    temp = temp.group_by(\"id\").agg(inter_key_largest_lantency = pl.max('time_diff'),\n",
    "                                   inter_key_median_lantency = pl.median('time_diff'),\n",
    "                                   mean_pause_time = pl.mean('time_diff'),\n",
    "                                   std_pause_time = pl.std('time_diff'),\n",
    "                                   total_pause_time = pl.sum('time_diff'),\n",
    "                                   pauses_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 0.5) & (pl.col('time_diff') < 1)).count(),\n",
    "                                   pauses_1_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1) & (pl.col('time_diff') < 1.5)).count(),\n",
    "                                   pauses_1_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1.5) & (pl.col('time_diff') < 2)).count(),\n",
    "                                   pauses_2_sec = pl.col('time_diff').filter((pl.col('time_diff') > 2) & (pl.col('time_diff') < 3)).count(),\n",
    "                                   pauses_3_sec = pl.col('time_diff').filter(pl.col('time_diff') > 3).count(),)\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "    \n",
    "    print(\"< P-bursts features >\")\n",
    "    #找到df中activity为‘Input’和‘Remove/cut’的行,并且是time_diff<2的行\n",
    "    temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n",
    "    temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n",
    "    temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "    temp = temp.with_columns(pl.col('time_diff')<2)\n",
    "    #然后统计连续出现的个数的(统计学变量)\n",
    "    temp = temp.with_columns(pl.when(pl.col(\"time_diff\") & pl.col(\"time_diff\").is_last()).then(pl.count()).over(pl.col(\"time_diff\").rle_id()).alias('P-bursts'))\n",
    "    temp = temp.drop_nulls()#删除包含缺失值的行\n",
    "    temp = temp.group_by(\"id\").agg(pl.mean('P-bursts').suffix('_mean'), pl.std('P-bursts').suffix('_std'), pl.count('P-bursts').suffix('_count'),\n",
    "                                   pl.median('P-bursts').suffix('_median'), pl.max('P-bursts').suffix('_max'),\n",
    "                                   pl.first('P-bursts').suffix('_first'), pl.last('P-bursts').suffix('_last'))\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "    print(\"< R-bursts features >\")\n",
    "    #取出数据中为'Remove/cut'\n",
    "    temp = df.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "    temp = temp.with_columns(pl.col('activity').is_in(['Remove/Cut']))\n",
    "    #统计'Remove/cut'连续出现的次数(的统计学变量)\n",
    "    temp = temp.with_columns(pl.when(pl.col(\"activity\") & pl.col(\"activity\").is_last()).then(pl.count()).over(pl.col(\"activity\").rle_id()).alias('R-bursts'))\n",
    "    temp = temp.drop_nulls()#删除包含缺失值的行\n",
    "    temp = temp.group_by(\"id\").agg(pl.mean('R-bursts').suffix('_mean'), pl.std('R-bursts').suffix('_std'), \n",
    "                                   pl.median('R-bursts').suffix('_median'), pl.max('R-bursts').suffix('_max'),\n",
    "                                   pl.first('R-bursts').suffix('_first'), pl.last('R-bursts').suffix('_last'))\n",
    "    feats = feats.join(temp, on='id', how='left')\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9046b1e-af4e-4f91-9795-0f131c51e9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "AGGREGATIONS = ['count', 'mean', 'min', 'max', 'first', 'last', q1, 'median', q3, 'sum']\n",
    "\n",
    "def word_feats(df):\n",
    "    essay_df = df\n",
    "    df['word'] = df['essay'].apply(lambda x: re.split(' |\\\\n|\\\\.|\\\\?|\\\\!',x))\n",
    "    df = df.explode('word')\n",
    "    df['word_len'] = df['word'].apply(lambda x: len(x))\n",
    "    df = df[df['word_len'] != 0]\n",
    "\n",
    "    word_agg_df = df[['id','word_len']].groupby(['id']).agg(AGGREGATIONS)\n",
    "    word_agg_df.columns = ['_'.join(x) for x in word_agg_df.columns]\n",
    "    word_agg_df['id'] = word_agg_df.index\n",
    "    word_agg_df = word_agg_df.reset_index(drop=True)\n",
    "    return word_agg_df\n",
    "\n",
    "\n",
    "def sent_feats(df):\n",
    "    df['sent'] = df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n",
    "    df = df.explode('sent')\n",
    "    df['sent'] = df['sent'].apply(lambda x: x.replace('\\n','').strip())\n",
    "    # Number of characters in sentences\n",
    "    df['sent_len'] = df['sent'].apply(lambda x: len(x))\n",
    "    # Number of words in sentences\n",
    "    df['sent_word_count'] = df['sent'].apply(lambda x: len(x.split(' ')))\n",
    "    df = df[df.sent_len!=0].reset_index(drop=True)\n",
    "\n",
    "    sent_agg_df = pd.concat([df[['id','sent_len']].groupby(['id']).agg(AGGREGATIONS), \n",
    "                             df[['id','sent_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1)\n",
    "    sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n",
    "    sent_agg_df['id'] = sent_agg_df.index\n",
    "    sent_agg_df = sent_agg_df.reset_index(drop=True)\n",
    "    sent_agg_df.drop(columns=[\"sent_word_count_count\"], inplace=True)\n",
    "    sent_agg_df = sent_agg_df.rename(columns={\"sent_len_count\":\"sent_count\"})\n",
    "    return sent_agg_df\n",
    "\n",
    "def parag_feats(df):\n",
    "    df['paragraph'] = df['essay'].apply(lambda x: x.split('\\n'))\n",
    "    df = df.explode('paragraph')\n",
    "    # Number of characters in paragraphs\n",
    "    df['paragraph_len'] = df['paragraph'].apply(lambda x: len(x)) \n",
    "    # Number of words in paragraphs\n",
    "    df['paragraph_word_count'] = df['paragraph'].apply(lambda x: len(x.split(' ')))\n",
    "    df = df[df.paragraph_len!=0].reset_index(drop=True)\n",
    "    \n",
    "    paragraph_agg_df = pd.concat([df[['id','paragraph_len']].groupby(['id']).agg(AGGREGATIONS), \n",
    "                                  df[['id','paragraph_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1) \n",
    "    paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n",
    "    paragraph_agg_df['id'] = paragraph_agg_df.index\n",
    "    paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n",
    "    paragraph_agg_df.drop(columns=[\"paragraph_word_count_count\"], inplace=True)\n",
    "    paragraph_agg_df = paragraph_agg_df.rename(columns={\"paragraph_len_count\":\"paragraph_count\"})\n",
    "    return paragraph_agg_df\n",
    "\n",
    "#论文的长度除以(‘Input’和‘Remove/Cut’)的按键个数.\n",
    "def product_to_keys(logs, essays):\n",
    "    essays['product_len'] = essays.essay.str.len()#论文的长度\n",
    "    #logs中每个id ‘Input’和‘Remove/Cut’的数据\n",
    "    tmp_df = logs[logs.activity.isin(['Input', 'Remove/Cut'])].groupby(['id']).agg({'activity': 'count'}).reset_index().rename(columns={'activity': 'keys_pressed'})\n",
    "    essays = essays.merge(tmp_df, on='id', how='left')\n",
    "    #论文的长度除以(‘Input’和‘Remove/Cut’)的按键个数.\n",
    "    essays['product_to_keys'] = essays['product_len'] / essays['keys_pressed']\n",
    "    return essays[['id', 'product_to_keys']]\n",
    "#统计每秒有几个['Input', 'Remove/Cut']的行为.\n",
    "def get_keys_pressed_per_second(logs):\n",
    "    #logs中为['Input', 'Remove/Cut']的event_id的个数\n",
    "    temp_df = logs[logs['activity'].isin(['Input', 'Remove/Cut'])].groupby(['id']).agg(keys_pressed=('event_id', 'count')).reset_index()\n",
    "    #每个id最小的down_time和最大的up_time\n",
    "    temp_df_2 = logs.groupby(['id']).agg(min_down_time=('down_time', 'min'), max_up_time=('up_time', 'max')).reset_index()\n",
    "    #按照id融合在一起\n",
    "    temp_df = temp_df.merge(temp_df_2, on='id', how='left')\n",
    "    #每秒有几个event_id\n",
    "    temp_df['keys_per_second'] = temp_df['keys_pressed'] / ((temp_df['max_up_time'] - temp_df['min_down_time']) / 1000)\n",
    "    return temp_df[['id', 'keys_per_second']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849b740a-1ff7-4219-9d01-8a70ba657045",
   "metadata": {},
   "outputs": [],
   "source": [
    "#传入训练数据dataX和datay,model,测试数据test_X\n",
    "#就是一个简单的k折交叉验证,不过模型只有1个训练5次.训练完就得到测试集的预测结果.\n",
    "def evaluate(data_x, data_y, model, random_state=seed, n_splits=5, test_x=None):\n",
    "    #StratifiedKFold还要考虑每种类别的占比\n",
    "    skf    = StratifiedKFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n",
    "    test_y = np.zeros((len(test_x), n_splits))#5折的预测结果\n",
    "    for i, (train_idx, valid_idx) in enumerate(skf.split(data_x, data_y.astype(str))):\n",
    "        train_x = data_x.iloc[train_idx]\n",
    "        train_y = data_y[train_idx]\n",
    "        valid_x = data_x.iloc[valid_idx]\n",
    "        valid_y = data_y[valid_idx]\n",
    "        model.fit(train_x, train_y)\n",
    "        test_y[:, i] = model.predict(test_x)\n",
    "    return np.mean(test_y, axis=1)\n",
    "\n",
    "\n",
    "train_logs    = pl.scan_csv('/kaggle/input/linking-writing-processes-to-writing-quality/train_logs.csv')\n",
    "train_feats   = dev_feats(train_logs)\n",
    "train_feats   = train_feats.collect().to_pandas()\n",
    "\n",
    "print('< Essay Reconstruction >')\n",
    "train_logs             = train_logs.collect().to_pandas()\n",
    "train_essays           = pd.read_csv('/kaggle/input/writing-quality-challenge-constructed-essays/train_essays_fast.csv')\n",
    "train_feats            = train_feats.merge(word_feats(train_essays), on='id', how='left')\n",
    "train_feats            = train_feats.merge(sent_feats(train_essays), on='id', how='left')\n",
    "train_feats            = train_feats.merge(parag_feats(train_essays), on='id', how='left')\n",
    "train_feats            = train_feats.merge(get_keys_pressed_per_second(train_logs), on='id', how='left')\n",
    "train_feats            = train_feats.merge(product_to_keys(train_logs, train_essays), on='id', how='left')\n",
    "\n",
    "#找到只有唯一值的列,删掉\n",
    "keys=train_feats.keys().values\n",
    "unique_cols=[key for key in keys if train_feats[key].nunique()<2]\n",
    "print(f\"unique_cols:{unique_cols}\")\n",
    "train_feats = train_feats.drop(columns=unique_cols)\n",
    "\n",
    "print('< Mapping >')\n",
    "train_scores   = pd.read_csv('/kaggle/input/linking-writing-processes-to-writing-quality/train_scores.csv')\n",
    "data           = train_feats.merge(train_scores, on='id', how='left')\n",
    "x              = data.drop(['id', 'score'], axis=1)\n",
    "y              = data['score'].values\n",
    "\n",
    "print(f'Number of features: {len(x.columns)}')\n",
    "\n",
    "print('< Testing Data >')\n",
    "test_logs   = pl.scan_csv('/kaggle/working/test_logs.csv')\n",
    "test_feats  = dev_feats(test_logs)\n",
    "test_feats  = test_feats.collect().to_pandas()\n",
    "\n",
    "test_logs             = test_logs.collect().to_pandas()\n",
    "test_essays           = test_essays_copy\n",
    "test_feats            = test_feats.merge(word_feats(test_essays), on='id', how='left')\n",
    "test_feats            = test_feats.merge(sent_feats(test_essays), on='id', how='left')\n",
    "test_feats            = test_feats.merge(parag_feats(test_essays), on='id', how='left')\n",
    "test_feats            = test_feats.merge(get_keys_pressed_per_second(test_logs), on='id', how='left')\n",
    "test_feats            = test_feats.merge(product_to_keys(test_logs, test_essays), on='id', how='left')\n",
    "\n",
    "test_feats = test_feats.drop(columns=unique_cols)\n",
    "\n",
    "test_ids = test_feats['id'].values\n",
    "testin_x = test_feats.drop(['id'], axis=1)\n",
    "\n",
    "print('< Learning and Evaluation >')\n",
    "lgbm_params = {'n_estimators': 1024,\n",
    "         'learning_rate': 0.006,\n",
    "         'metric': 'rmse',\n",
    "         'random_state': seed,\n",
    "         'force_col_wise': True,\n",
    "         'verbosity': 0,}\n",
    "solution = LGBMRegressor(**lgbm_params)\n",
    "y_pred_lgb   = evaluate(x.copy(), y.copy(), solution, test_x=testin_x.copy()) \n",
    "y_pred_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21de058d-3db4-441b-a8b4-aab9cc17cd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = blended_score*0.6+ y_pred_lgb*0.4#将两种预测结果进行一个加权融合\n",
    "\n",
    "submission = pd.DataFrame({'id': test_ids, 'score': y_pred})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
